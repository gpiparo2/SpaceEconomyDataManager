{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67455862",
   "metadata": {},
   "source": [
    "# Test Download Code\n",
    "\n",
    "This notebook tests the core functionality of the Satellite Data Manager library by calling its methods directly. It sets up all the available options (download parameters, sensor selections, band configurations, cropping, masking, etc.) as used in the interactive GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68343d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sentinelhub import SHConfig\n",
    "from SatelliteDataManager.core.sdm import SDM\n",
    "from SatelliteDataManager.analyses.SEDM_Wildfire.burned_area_dataset_builder import BurnedAreaSegmentationDatasetBuilder\n",
    "from SatelliteDataManager.analyses.SEDM_Wildfire.burned_area_model import build_burned_area_segmentation_model\n",
    "from SatelliteDataManager.analyses.SEDM_Wildfire.burned_area_optuna import objective as burned_area_objective\n",
    "\n",
    "\n",
    "# Import splitting and visualization utilities\n",
    "from SatelliteDataManager.core.ml.data_split import train_test_split, stratified_train_test_split, kfold_split, stratified_kfold_split\n",
    "from SatelliteDataManager.core.ml.hyperparameter_optimization import run_optuna_study\n",
    "from SatelliteDataManager.core.ml.result_visualizer import plot_training_history, plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439bc229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and directories set up.\n",
      "BurnedAreaSegmentationDatasetBuilder initialized.\n"
     ]
    }
   ],
   "source": [
    "config = SHConfig(\"peppe\")\n",
    "\n",
    "\n",
    "# Define paths for activation info and data directories\n",
    "activation_info_path = \"FireData/activation_info.json\"  # JSON file with activation information\n",
    "base_data_folder = \"./test/raw\"           # Directory for raw downloads\n",
    "base_manipulated_folder = \"./test/man\"      # Directory for organized data\n",
    "tfrecord_folder = \"./test/tfrecords\"         # Directory for saving TFRecord files\n",
    "\n",
    "print(\"Configuration and directories set up.\")\n",
    "\n",
    "# Initialize the BurnedAreaSegmentationDatasetBuilder with all parameters\n",
    "builder = BurnedAreaSegmentationDatasetBuilder(\n",
    "    config=config,\n",
    "    activation_info_path=activation_info_path,\n",
    "    base_data_folder=base_data_folder,\n",
    "    base_manipulated_folder=base_manipulated_folder,\n",
    "    tfrecord_folder=tfrecord_folder,\n",
    "    sampleType=\"FLOAT32\",\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(\"BurnedAreaSegmentationDatasetBuilder initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fe755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download parameters defined.\n"
     ]
    }
   ],
   "source": [
    "# Define per-satellite download parameters\n",
    "download_params = {\n",
    "    \"interval_days\": {\n",
    "         \"Sentinel-2\": 12,\n",
    "         \"Sentinel-1\": 12,\n",
    "         \"Sentinel-3-OLCI\": 3,\n",
    "         \"Sentinel-3-SLSTR-Thermal\": 3,\n",
    "         \"DEM\": 36\n",
    "    },\n",
    "    \"size\": {\n",
    "         \"default\": (256, 256),\n",
    "         \"Sentinel-2\": (256, 256),\n",
    "         \"Sentinel-1\": (256, 256),\n",
    "         \"Sentinel-3-OLCI\": (256, 256),\n",
    "         \"Sentinel-3-SLSTR-Thermal\": (256, 256),\n",
    "         \"DEM\": (256, 256)\n",
    "    },\n",
    "    \"mosaicking_order\": {\n",
    "         \"Sentinel-2\": \"leastCC\",\n",
    "         \"Sentinel-1\": \"mostRecent\",\n",
    "         \"Sentinel-3-OLCI\": \"mostRecent\",\n",
    "         \"Sentinel-3-SLSTR-Thermal\": \"mostRecent\",\n",
    "         \"DEM\": \"mostRecent\"\n",
    "    },\n",
    "    \"resolutions\": {\n",
    "         \"default\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Download parameters defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9849b785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional parameters set.\n"
     ]
    }
   ],
   "source": [
    "# Define additional parameters for testing\n",
    "\n",
    "# Specify which sensors to process\n",
    "sensors = [\"Sentinel-2\", \"Sentinel-1\", \"Sentinel-3-OLCI\", \"Sentinel-3-SLSTR-Thermal\", \"DEM\"]\n",
    "\n",
    "# Specify per-sensor band selection parameters (optional)\n",
    "sensor_bands = {\n",
    "    \"Sentinel-2\": {\"bands\": [\"B02\", \"B03\", \"B04\", \"B8A\"]},\n",
    "    \"Sentinel-1\": {\"polarizations\": [\"VV\", \"VH\"]}\n",
    "}\n",
    "\n",
    "# Specify per-sensor download (evalscript) parameters (optional)\n",
    "evalscript_params = {\n",
    "    \"Sentinel-2\": {\"units\": \"REFLECTANCE\", \"sampleType\": \"FLOAT32\"},\n",
    "    \"Sentinel-1\": {\"polarizations\": [\"VV\", \"VH\"], \"backCoeff\": \"GAMMA0_ELLIPSOID\"}\n",
    "}\n",
    "\n",
    "fire_info_base_folder = \"./\"  \n",
    "\n",
    "print(\"Additional parameters set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917c56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SatelliteDataManager.core.data_download:**STARTING DATA DOWNLOAD**\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-2 images for interval: 2019-04-27 to 2019-05-08 (1/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing activation: EMSR353-AOI01\n",
      "Skipping activation EMSR353-AOI01: Fire GeoJSON not found.\n",
      "Processing activation: EMSR360-AOI01\n",
      "Interval days dict: {'Sentinel-2': 12, 'Sentinel-1': 12, 'Sentinel-3-OLCI': 3, 'Sentinel-3-SLSTR-Thermal': 3, 'DEM': 36}\n",
      "Size dict: {'Sentinel-2': (256, 256), 'Sentinel-1': (256, 256), 'Sentinel-3-OLCI': (256, 256), 'Sentinel-3-SLSTR-Thermal': (256, 256), 'DEM': (256, 256)}\n",
      "Mosaicking order dict: {'Sentinel-2': 'leastCC', 'Sentinel-1': 'mostRecent', 'Sentinel-3-OLCI': 'mostRecent', 'Sentinel-3-SLSTR-Thermal': 'mostRecent', 'DEM': 'mostRecent'}\n",
      "Date from dict: {'Sentinel-2': datetime.datetime(2019, 4, 27, 0, 0), 'Sentinel-1': datetime.datetime(2019, 4, 27, 0, 0), 'Sentinel-3-OLCI': datetime.datetime(2019, 4, 27, 0, 0), 'Sentinel-3-SLSTR-Thermal': datetime.datetime(2019, 4, 27, 0, 0), 'DEM': datetime.datetime(2019, 4, 27, 0, 0)}\n",
      "Date to dict: {'Sentinel-2': datetime.datetime(2019, 6, 2, 0, 0), 'Sentinel-1': datetime.datetime(2019, 6, 2, 0, 0), 'Sentinel-3-OLCI': datetime.datetime(2019, 6, 2, 0, 0), 'Sentinel-3-SLSTR-Thermal': datetime.datetime(2019, 6, 2, 0, 0), 'DEM': datetime.datetime(2019, 6, 2, 0, 0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-2 images for interval: 2019-05-09 to 2019-05-20 (2/3)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-2 images for interval: 2019-05-21 to 2019-06-01 (3/3)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-1 images for interval: 2019-04-27 to 2019-05-08 (1/3)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-1 images for interval: 2019-05-09 to 2019-05-20 (2/3)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-1 images for interval: 2019-05-21 to 2019-06-01 (3/3)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-3-OLCI images for interval: 2019-04-27 to 2019-04-29 (1/12)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-3-OLCI images for interval: 2019-04-30 to 2019-05-02 (2/12)\n",
      "INFO:SatelliteDataManager.core.data_download:Data downloaded and saved.\n",
      "INFO:SatelliteDataManager.core.data_download:Downloading Sentinel-3-OLCI images for interval: 2019-05-03 to 2019-05-05 (3/12)\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset for all activations\n",
    "# Specify the base folder for fire information (GeoJSON files)\n",
    "fire_info_base_folder = \"./\"  \n",
    "# For this test, process 1 activation, with a time window of 18 days around the fire date,\n",
    "# and apply normalization, masking, and cropping (crop_factor=4).\n",
    "builder.build_dataset_for_all_activations(\n",
    "    max_activations=2,\n",
    "    use_skip_list=True,\n",
    "    time_window=18,\n",
    "    download_params=download_params,\n",
    "    apply_normalization=True,\n",
    "    apply_mask=True,\n",
    "    crop=True,\n",
    "    crop_factor=2,\n",
    "    #sensors=sensors,\n",
    "    #sensor_bands=sensor_bands,\n",
    "    #evalscript_params=evalscript_params,\n",
    "    fire_info_base_folder=fire_info_base_folder\n",
    ")\n",
    "\n",
    "print(\"Dataset building complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization test\n",
    "import os\n",
    "from SatelliteDataManager.core.sdm import SDM\n",
    "\n",
    "# Initialize SDM instance for visualization\n",
    "sdm = SDM(\n",
    "    config=config,\n",
    "    data_folder=base_data_folder,\n",
    "    manipulated_folder=base_manipulated_folder,\n",
    "    tfrecord_folder=tfrecord_folder\n",
    ")\n",
    "\n",
    "# List all TFRecord files generated\n",
    "tfrecord_files = sorted([os.path.join(tfrecord_folder, f) \n",
    "                         for f in os.listdir(tfrecord_folder) \n",
    "                         if f.endswith(\".tfrecord\")])\n",
    "print(\"TFRecord files:\", tfrecord_files)\n",
    "\n",
    "# For visualization, select the first TFRecord file (if available) and visualize it\n",
    "if tfrecord_files:\n",
    "    file_to_visualize = tfrecord_files[0]\n",
    "    print(f\"Visualizing TFRecord file: {file_to_visualize}\")\n",
    "    #sdm.data_visualizer.inspect_and_visualize_custom_tfrecord(file_to_visualize, crop=True, crop_factor=2)\n",
    "else:\n",
    "    print(\"No TFRecord files found for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Import necessary modules\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import the required classes from the library\n",
    "from SatelliteDataManager.analyses.SEDM_Wildfire.burned_area_dataset_builder import BurnedAreaSegmentationDatasetBuilder\n",
    "from SatelliteDataManager.core.sdm import SDM\n",
    "from sentinelhub import SHConfig\n",
    "# Initialize Sentinel Hub configuration\n",
    "config = SHConfig(\"peppe\")\n",
    "\n",
    "# Initialize the SDM instance with the appropriate folders\n",
    "sdm = SDM(\n",
    "    config=config,\n",
    "    data_folder=\"./test/raw\",\n",
    "    manipulated_folder=\"./test/man\",\n",
    "    tfrecord_folder=\"./test/tfrecords\"\n",
    ")\n",
    "\n",
    "# Get the list of TFRecord files from the specified folder\n",
    "tfrecord_files = glob.glob(\"data/tfrecords/*.tfrecord\")\n",
    "\n",
    "# Define the list of sensors to include (if None, all default sensors are used)\n",
    "selected_sensors = [\"Sentinel-2\", \"Sentinel-1\"]\n",
    "\n",
    "# Create the dataset using the new get_dataset method with sensor selection, cropping, augmentation, etc.\n",
    "dataset = sdm.dataset_preparer.get_dataset(\n",
    "    tfrecord_files=tfrecord_files,\n",
    "    batch_size=16,\n",
    "    augment=True,\n",
    "    crop=True,\n",
    "    crop_factor=2,\n",
    "    min_image_nonzero_percentage=0.2,\n",
    "    min_label_nonzero_percentage=0.2,\n",
    "    sensors=selected_sensors\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset characteristics (stampa solo i sensori selezionati)\n",
    "sdm.dataset_preparer.print_dataset_characteristics(dataset, sensors=selected_sensors)\n",
    "import numpy as np\n",
    "\n",
    "# Retrieve one batch from the dataset\n",
    "for batch_inputs, batch_labels in dataset.take(1):\n",
    "    # Get the Sentinel-1 images from the batch.\n",
    "    # Assuming the shape is (batch_size, n_steps, height, width, channels)\n",
    "    sentinel1_images = batch_inputs[\"Sentinel-2\"].numpy()\n",
    "    \n",
    "    # Check if there are at least three time steps.\n",
    "    if sentinel1_images.shape[1] < 3:\n",
    "        print(\"Not enough temporal steps for Sentinel-1 data.\")\n",
    "    else:\n",
    "        for i in range(sentinel1_images.shape[0]):\n",
    "            step0 = sentinel1_images[i, 0]\n",
    "            step1 = sentinel1_images[i, 1]\n",
    "            step2 = sentinel1_images[i, 2]\n",
    "            \n",
    "            # Use np.array_equal to check if the arrays are exactly equal.\n",
    "            if np.array_equal(step0, step1) and np.array_equal(step1, step2):\n",
    "                print(f\"Example {i}: All three temporal steps for Sentinel-1 are equal.\")\n",
    "            else:\n",
    "                print(f\"Example {i}: Temporal steps differ for Sentinel-1.\")\n",
    "\n",
    "\n",
    "# Visualize one batch of the dataset using the visualizer, mostrando solo i sensori selezionati.\n",
    "sdm.data_visualizer.visualize_batch(dataset, sensors_to_show=selected_sensors, max_examples=1, crop=True, crop_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d9bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# STEP 1: SPLIT INTO TRAIN AND TEST (WITH OPTIONAL STRATIFICATION)\n",
    "# --------------------------\n",
    "# For demonstration, we simulate a label (e.g., binary indicator) for each TFRecord file.\n",
    "# In practice, these labels should be extracted from metadata.\n",
    "\n",
    "# List all TFRecord files generated\n",
    "tfrecord_files = sorted([os.path.join(tfrecord_folder, f) \n",
    "                         for f in os.listdir(tfrecord_folder) \n",
    "                         if f.endswith(\".tfrecord\")])\n",
    "\n",
    "\n",
    "file_array = np.array(tfrecord_files)\n",
    "# Simulate labels: random 0 or 1 for each file\n",
    "simulated_labels = np.random.randint(0, 2, size=len(file_array))\n",
    "\n",
    "# Set this flag to True to stratify, or False for a simple random split.\n",
    "stratify_split = False # Set to True for stratified split TO BE FIXED\n",
    "test_size = 0.25\n",
    "\n",
    "\n",
    "if stratify_split:\n",
    "    # Using stratified split based on simulated_labels\n",
    "    from SatelliteDataManager.core.ml.data_split import stratified_train_test_split\n",
    "    train_files, test_files, train_labels, test_labels = stratified_train_test_split(file_array, simulated_labels, test_size=test_size, random_state=42)\n",
    "    print(f\"Stratified split: {len(train_files)} train files, {len(test_files)} test files\")\n",
    "else:\n",
    "    # Simple random split\n",
    "    from SatelliteDataManager.core.ml.data_split import train_test_split\n",
    "    train_files, test_files = train_test_split(file_array, test_size=test_size, random_state=42)\n",
    "    print(f\"Random split: {len(train_files)} train files, {len(test_files)} test files\")\n",
    "\n",
    "# Create datasets from the file lists\n",
    "sdm = SDM(\n",
    "    config=config,\n",
    "    data_folder=base_data_folder,\n",
    "    manipulated_folder=base_manipulated_folder,\n",
    "    tfrecord_folder=tfrecord_folder\n",
    ")\n",
    "\n",
    "train_ds = sdm.dataset_preparer.get_dataset(\n",
    "    tfrecord_files=train_files.tolist(),\n",
    "    batch_size=8,\n",
    "    augment=True,\n",
    "    crop=True,\n",
    "    crop_factor=2,\n",
    "    min_label_percentage=0.\n",
    ")\n",
    "test_ds = sdm.dataset_preparer.get_dataset(\n",
    "    tfrecord_files=test_files.tolist(),\n",
    "    batch_size=8,\n",
    "    augment=False,\n",
    "    crop=True,\n",
    "    crop_factor=2,\n",
    "    min_label_percentage=0.\n",
    ")\n",
    "print(\"Train and test datasets created.\")\n",
    "\n",
    "# --------------------------\n",
    "# Define input shapes (must match dataset creation)\n",
    "input_shapes = {\n",
    "    \"Sentinel-2\": (3, 128, 128, 17),\n",
    "    \"Sentinel-1\": (3, 128, 128, 2),\n",
    "    \"Sentinel-3-OLCI\": (12, 128, 128, 21),\n",
    "    \"Sentinel-3-SLSTR-Thermal\": (12, 128, 128, 5),\n",
    "    \"DEM\": (128, 128, 1)\n",
    "}\n",
    "\n",
    "\n",
    "#sdm.dataset_preparer.print_dataset_characteristics(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# STEP 2: HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
    "# --------------------------\n",
    "# Run a short Optuna study \n",
    "study = run_optuna_study(\n",
    "    objective=lambda trial: burned_area_objective(trial, input_shapes, train_ds, test_ds, epochs=5),\n",
    "    n_trials=5,\n",
    "    study_name=\"burned_area_optimization\",\n",
    "    direction=\"minimize\"\n",
    ")\n",
    "best_params = study.best_params\n",
    "print(\"Optuna study complete. Best hyperparameters found:\", best_params)\n",
    "\n",
    "# Build the best model using the best parameters\n",
    "best_model = build_burned_area_segmentation_model(\n",
    "    input_shapes,\n",
    "    dropout_rate=best_params[\"dropout_rate\"],\n",
    "    l2_reg=best_params[\"l2_reg\"],\n",
    "    s2_filters1=best_params[\"s2_filters1\"],\n",
    "    s2_filters2=best_params[\"s2_filters2\"],\n",
    "    s1_filters1=best_params[\"s1_filters1\"],\n",
    "    s1_filters2=best_params[\"s1_filters2\"],\n",
    "    s3olci_filters1=best_params[\"s3olci_filters1\"],\n",
    "    s3olci_filters2=best_params[\"s3olci_filters2\"],\n",
    "    s3slstr_filters1=best_params[\"s3slstr_filters1\"],\n",
    "    s3slstr_filters2=best_params[\"s3slstr_filters2\"],\n",
    "    dem_filters1=best_params[\"dem_filters1\"],\n",
    "    dem_filters2=best_params[\"dem_filters2\"]\n",
    ")\n",
    "best_model.summary()\n",
    "\n",
    "# Train best model on the entire train set and evaluate on test set\n",
    "history_best = best_model.fit(train_ds, validation_data=test_ds, epochs=10)\n",
    "print(\"Best model training complete.\")\n",
    "\n",
    "# Save best model weights (in .keras format) and training history\n",
    "best_model.save(\"best_burned_area_model.keras\")\n",
    "with open(\"best_model_history.json\", \"w\") as f:\n",
    "    json.dump(history_best.history, f)\n",
    "print(\"Best model and training history saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# STEP 3: K-FOLD CROSS VALIDATION ON TRAINING SET\n",
    "# --------------------------\n",
    "# For k-fold, we split the train_files array. We use stratified_kfold_split if stratification is desired.\n",
    "from SatelliteDataManager.core.ml.data_split import kfold_split, stratified_kfold_split\n",
    "\n",
    "k = 5  # number of folds\n",
    "if stratify_split:\n",
    "    # Use simulated_labels corresponding to train_files for stratification\n",
    "    train_labels_for_fold = [int(x) for x in train_labels]\n",
    "    fold_splits = list(stratified_kfold_split(train_files, train_labels_for_fold, k=k, random_state=42))\n",
    "else:\n",
    "    fold_splits = list(kfold_split(train_files, k=k, random_state=42))\n",
    "\n",
    "fold_histories = []\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(fold_splits):\n",
    "    print(f\"\\nFold {fold+1}/{k}\")\n",
    "    fold_train_files = train_files[train_idx]\n",
    "    fold_val_files = train_files[val_idx]\n",
    "    \n",
    "    # Create fold datasets\n",
    "    fold_train_ds = sdm.dataset_preparer.get_dataset(\n",
    "        tfrecord_files=fold_train_files.tolist(),\n",
    "        batch_size=16,\n",
    "        augment=True,\n",
    "        crop=True,\n",
    "        crop_factor=4,\n",
    "        min_label_percentage=0.\n",
    "    )\n",
    "    fold_val_ds = sdm.dataset_preparer.get_dataset(\n",
    "        tfrecord_files=fold_val_files.tolist(),\n",
    "        batch_size=16,\n",
    "        augment=False,\n",
    "        crop=True,\n",
    "        crop_factor=4,\n",
    "        min_label_percentage=0.\n",
    "    )\n",
    "    \n",
    "    # Build a new model with the best hyperparameters\n",
    "    fold_model = build_burned_area_segmentation_model(\n",
    "        input_shapes,\n",
    "        dropout_rate=best_params[\"dropout_rate\"],\n",
    "        l2_reg=best_params[\"l2_reg\"],\n",
    "        s2_filters1=best_params[\"s2_filters1\"],\n",
    "        s2_filters2=best_params[\"s2_filters2\"],\n",
    "        s1_filters1=best_params[\"s1_filters1\"],\n",
    "        s1_filters2=best_params[\"s1_filters2\"],\n",
    "        s3olci_filters1=best_params[\"s3olci_filters1\"],\n",
    "        s3olci_filters2=best_params[\"s3olci_filters2\"],\n",
    "        s3slstr_filters1=best_params[\"s3slstr_filters1\"],\n",
    "        s3slstr_filters2=best_params[\"s3slstr_filters2\"],\n",
    "        dem_filters1=best_params[\"dem_filters1\"],\n",
    "        dem_filters2=best_params[\"dem_filters2\"]\n",
    "    )\n",
    "    \n",
    "    # Train for a fixed number of epochs (e.g., 5 for demo)\n",
    "    history_fold = fold_model.fit(fold_train_ds, validation_data=fold_val_ds, epochs=5, verbose=0)\n",
    "    fold_histories.append(history_fold.history)\n",
    "    \n",
    "    # Evaluate fold model on validation fold\n",
    "    metrics_fold = fold_model.evaluate(fold_val_ds, verbose=0)\n",
    "    fold_metrics.append(dict(zip(fold_model.metrics_names, metrics_fold)))\n",
    "    print(f\"Fold {fold+1} metrics:\", fold_metrics[-1])\n",
    "\n",
    "# Optionally, aggregate and print average metrics across folds\n",
    "avg_metrics = {}\n",
    "for metric in fold_metrics[0].keys():\n",
    "    avg_metrics[metric] = np.mean([fold[metric] for fold in fold_metrics])\n",
    "print(\"\\nAverage k-fold metrics:\", avg_metrics)\n",
    "\n",
    "# --------------------------\n",
    "# FINAL EVALUATION: BEST MODEL ON TRAIN/TEST SPLIT\n",
    "# --------------------------\n",
    "# Evaluate the best model (obtained via Optuna) on train and test sets separately\n",
    "train_eval = best_model.evaluate(train_ds, verbose=0)\n",
    "test_eval = best_model.evaluate(test_ds, verbose=0)\n",
    "print(\"\\nBest Model Evaluation:\")\n",
    "print(\"Train set metrics:\", dict(zip(best_model.metrics_names, train_eval)))\n",
    "print(\"Test set metrics:\", dict(zip(best_model.metrics_names, test_eval)))\n",
    "\n",
    "# Plot training history and ROC curve for final best model\n",
    "plot_training_history(history_best)\n",
    "\n",
    "all_labels_test = []\n",
    "all_preds_test = []\n",
    "for batch_inputs, batch_labels in test_ds:\n",
    "    preds = best_model.predict(batch_inputs)\n",
    "    all_labels_test.append(batch_labels.numpy().flatten())\n",
    "    all_preds_test.append(preds.flatten())\n",
    "all_labels_test = np.concatenate(all_labels_test)\n",
    "all_preds_test = np.concatenate(all_preds_test)\n",
    "plot_roc_curve(all_labels_test, all_preds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
